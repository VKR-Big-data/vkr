# ВКР. Исследование и сравнительный анализ средств работы с большими данными в веб-приложениях
В рамках ВКР передо мной стояла задача сравнения средств работы с большими данными. Ультимативным критерием стала временная эффективность выполнения задач(задержка для потоковой обработки и время выполнения для пакетной), однако оценивались также и другие харакетристики средств.
Изучив вопрос, было принято решение в рамках работы разделить сравниваемые средства на две категории: средства для потоковой обработки данных и средства для пакетной обработки данных - это две фундаментальные для анализа больших данных задачи, при этом практически технологически противоположные. 
## Сраниваемые технологии
В качества сравниваемых средств для потоковой обработки данных были выбраны Apache Flink и Spark Streaming. Это два мощных и популярных инструмента, обладающие различными интерфейсами для написания задач потоковой обработки данных, при этом их сравнение однозначно имеет смысл, так как они реализуют разный архитектурный подход к потоковйо обработке данных: Apache Flink обрабатывает данные истинно потоково, а Spark Streaming - микро-пакетно.
Для пакетной обработки данных были выбраны Spark, Hive on Tez и Map Reduce. Звучит не совсем корректно, поэтому нужно пояснить: Hive On Tez - стандартное для индустрии сочетание движка и интерфейса, таким образом с точки зрения временной эффектисности в оценке участвовал Tez, с точки зрения удобства интерфейса - Hive. Map Reduce задачи также писались на Hive, но в качестве движка при выполнении был именно MR.
## Вспомогательные технологии
Чтобы сравнивать средства пакетной обработки нам нужно было хранлище, в котором бы хранились данные, к которым мы бы выполняли запросы. В качестве такого хранлища был выбран стандартный компонент Apache Hadoop - HDFS. Сравниваемые фреймворки являются проектами верхнего уровня Apache(или подпроектами проектов верхнего уровня), поэтому обладают тесной интеграцией с Hadoop в целом и с HDFS в частности.
Касаемо потоковой обработки данных нам нужен был источник сообщений для Flink и Spark Streaming работ, который бы обеспечивал целостность входных данных. В качестве такого источника входных данных для работ был выбран Apache Kafka, так как он также тесно интегрирован с обоими сравниваемыми средствами потоковой обработки данных.
## Данные
Для проведение экспериментального сравнения нужен был источник данных, который подходит и для пакетной и для потоковой обработки. В качестве такого источника были выбраны чаты стриминговой платформы [Twitch](https://www.twitch.tv/). Этот исчтоник может считаться источником больших потоковых данных, как минимум количественно, так как количество приходящих по вебсокету сообщений при достаточном количестве зрителей [получается внушительным](https://stats.streamelements.com/), кроме того, можно подключаться к нескольким чатам, тем самым увеличивая скорость поступления потоковых данных в систему. Программно данные получаются с помощью IRC сокетов, для менеджемента которых в момент проведения эксперимента был создан специальный REST API сервер.
## Пайплайн
![Диаграмма потоков данных в рамках системы](https://github.com/guaNa228/vkr/blob/master/%D0%98%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F/%D0%9F%D0%BE%D1%82%D0%BE%D0%BA%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.png?raw=true)
### Результаты
### Код
